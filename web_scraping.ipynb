{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c0c87c-2ded-41fc-9412-ac20cfe888aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.2)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.24.0-py3-none-any.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.35.22-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
      "Collecting typing_extensions~=4.9\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Collecting websocket-client~=1.8\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trio~=0.17\n",
      "  Downloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.36.0,>=1.35.22\n",
      "  Downloading botocore-1.35.22-py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.22->boto3) (2.8.2)\n",
      "Collecting attrs>=23.2.0\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Collecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.22->boto3) (1.16.0)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sortedcontainers, websocket-client, typing_extensions, pysocks, jmespath, h11, attrs, wsproto, outcome, botocore, trio, s3transfer, trio-websocket, boto3, selenium\n",
      "\u001b[33m  WARNING: The script wsdump is installed in '/home/workbench/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed attrs-24.2.0 boto3-1.35.22 botocore-1.35.22 h11-0.14.0 jmespath-1.0.1 outcome-1.3.0.post0 pysocks-1.7.1 s3transfer-0.10.2 selenium-4.24.0 sortedcontainers-2.4.0 trio-0.26.2 trio-websocket-0.11.1 typing_extensions-4.12.2 websocket-client-1.8.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 selenium boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a51a8575-7899-494b-ac44-d9134ec3008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] += os.pathsep + '/home/workbench/.local/bin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4eaa94-4b65-4711-b12b-668bd1a3fafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/workbench/.local/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['PATH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dab06f4f-21ea-4505-94e2-72b662b47228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the job postings page\n",
    "url = \"https://www.indeed.com/jobs?q=software+engineer&l=chicago\"  # Replace with the actual URL you want to scrape\n",
    "\n",
    "# Send a GET request to fetch the page content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the page content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find job postings (Assuming job listings are inside a class named 'job-listing')\n",
    "jobs = soup.find_all('div', class_='job-listing')\n",
    "\n",
    "# Extract job details\n",
    "for job in jobs:\n",
    "    title = job.find('h2').text  # Assuming job title is inside <h2> tags\n",
    "    company = job.find('h3').text  # Assuming company name is inside <h3> tags\n",
    "    description = job.find('p').text  # Assuming description is inside <p> tags\n",
    "    print(f\"Job Title: {title}, Company: {company}, Description: {description}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22704940-aee7-4de4-afa2-2a54139ae32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully scraped from https://www.careerjet.com/search/jobs?s=software+engineer&l=chicago\n",
      "Data successfully scraped from https://stackoverflow.com/jobs?q=python&l=chicago\n",
      "Data successfully scraped from https://www.indeed.com/jobs?q=software+engineer&l=chicago\n",
      "Data written to jobs.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    'https://www.careerjet.com/search/jobs?s=software+engineer&l=chicago',\n",
    "    'https://stackoverflow.com/jobs?q=python&l=chicago',\n",
    "    'https://www.indeed.com/jobs?q=software+engineer&l=chicago'# Replace with actual URL\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# Open a CSV file and write the headers\n",
    "with open('jobs.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Job Title', 'Company', 'Location'])  # CSV headers\n",
    "\n",
    "    # Loop through each URL\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all job postings (adjust the selector as needed)\n",
    "            jobs = soup.find_all('article', class_='job')\n",
    "\n",
    "            # Loop through each job and extract details\n",
    "            for job in jobs:\n",
    "                try:\n",
    "                    # Check if the title, company, and location exist before accessing them\n",
    "                    title = job.find('h2').text.strip() if job.find('h2') else 'N/A'\n",
    "                    company = job.find('p', class_='company').text.strip() if job.find('p', class_='company') else 'N/A'\n",
    "                    location = job.find('p', class_='location').text.strip() if job.find('p', class_='location') else 'N/A'\n",
    "\n",
    "                    # Write the job details to the CSV file\n",
    "                    writer.writerow([title, company, location])\n",
    "                except AttributeError:\n",
    "                    # Handle cases where an element might be missing\n",
    "                    print(f\"Error scraping one of the job entries from URL: {url}\")\n",
    "                    continue\n",
    "\n",
    "            print(f\"Data successfully scraped from {url}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle errors related to URL fetching or parsing\n",
    "            print(f\"Error fetching or parsing URL: {url}\")\n",
    "            print(e)\n",
    "\n",
    "print(\"Data written to jobs.csv successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bbade31-75c7-41b4-8dc6-accd32849009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully scraped from https://www.careerjet.com/search/jobs?s=software+engineer&l=chicago\n",
      "Data successfully scraped from https://www.indeed.com/q-Software-Engineer-l-Chicago,-IL-jobs.html\n",
      "Data successfully scraped from https://stackoverflow.com/jobs?sort=p\n",
      "Data written to jobs.csv successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    'https://www.careerjet.com/search/jobs?s=software+engineer&l=chicago',\n",
    "    'https://www.indeed.com/q-Software-Engineer-l-Chicago,-IL-jobs.html',  # Replace with actual URL\n",
    "    'https://stackoverflow.com/jobs?sort=p'  # Replace with actual URL\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# Function to parse job postings from a specific URL\n",
    "def parse_jobs(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Adjust the selectors based on the site’s structure\n",
    "        jobs = []\n",
    "        if 'careerjet' in url:\n",
    "            # Example for Careerjet\n",
    "            job_elements = soup.find_all('article', class_='job')\n",
    "            for job in job_elements:\n",
    "                title = job.find('h2').text.strip() if job.find('h2') else 'N/A'\n",
    "                company = job.find('p', class_='company').text.strip() if job.find('p', class_='company') else 'N/A'\n",
    "                location = job.find('p', class_='location').text.strip() if job.find('p', class_='location') else 'N/A'\n",
    "                jobs.append([title, company, location])\n",
    "        \n",
    "        elif 'indeed' in url:\n",
    "            # Example for Indeed\n",
    "            job_elements = soup.find_all('div', class_='jobsearch-SerpJobCard')\n",
    "            for job in job_elements:\n",
    "                title = job.find('a', class_='jobtitle').text.strip() if job.find('a', class_='jobtitle') else 'N/A'\n",
    "                company = job.find('span', class_='company').text.strip() if job.find('span', class_='company') else 'N/A'\n",
    "                location = job.find('div', class_='location').text.strip() if job.find('div', class_='location') else 'N/A'\n",
    "                jobs.append([title, company, location])\n",
    "        \n",
    "        elif 'stackoverflow' in url:\n",
    "            # Example for Stack Overflow\n",
    "            job_elements = soup.find_all('div', class_='js-result')\n",
    "            for job in job_elements:\n",
    "                title = job.find('a', class_='s-link').text.strip() if job.find('a', class_='s-link') else 'N/A'\n",
    "                company = job.find('div', class_='fc-black-700').text.strip() if job.find('div', class_='fc-black-700') else 'N/A'\n",
    "                location = job.find('span', class_='fc-black-500').text.strip() if job.find('span', class_='fc-black-500') else 'N/A'\n",
    "                jobs.append([title, company, location])\n",
    "        \n",
    "        return jobs\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching or parsing URL: {url}\")\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "# Open a CSV file and write the headers\n",
    "with open('jobs.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Job Title', 'Company', 'Location'])  # CSV headers\n",
    "\n",
    "    # Loop through each URL\n",
    "    for url in urls:\n",
    "        jobs = parse_jobs(url)\n",
    "        for job in jobs:\n",
    "            writer.writerow(job)\n",
    "        print(f\"Data successfully scraped from {url}\")\n",
    "\n",
    "print(\"Data written to jobs.csv successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09670f58-292a-4d85-928f-a985d3076cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
